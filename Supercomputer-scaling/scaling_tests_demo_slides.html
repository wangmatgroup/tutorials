<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="author" content="Wang Materials Group, UT Austin">
  <title>Tutorial: Scaling tests</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="https://unpkg.com/reveal.js@3.9.2//css/reveal.css">
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
  </style>
  <link rel="stylesheet" href="https://unpkg.com/reveal.js@3.9.2//css/theme/white.css" id="theme">
  <!-- Printing and PDF exports -->
  <script>
    var link = document.createElement( 'link' );
    link.rel = 'stylesheet';
    link.type = 'text/css';
    link.href = window.location.search.match( /print-pdf/gi ) ? 'https://unpkg.com/reveal.js@3.9.2//css/print/pdf.css' : 'https://unpkg.com/reveal.js@3.9.2//css/print/paper.css';
    document.getElementsByTagName( 'head' )[0].appendChild( link );
  </script>
  <!--[if lt IE 9]>
  <script src="https://unpkg.com/reveal.js@3.9.2//lib/js/html5shiv.js"></script>
  <![endif]-->
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
</head>
<body>
  <div class="reveal">
    <div class="slides">

<section>
  <h1 class="title">Tutorial:<br />
Scaling tests</h1>
  <p class="author">Wang Materials Group, UT Austin</p>
  <p class="date">Updated: June 2023</p>
</section>

<section id="concepts-for-today" class="slide level2">
<h2>Concepts for today</h2>
<ul>
<li>Basics of parallelization</li>
<li>Jobs on the supercomputer</li>
<li>What are scaling tests</li>
<li>Why do we do scaling tests</li>
<li>How do we do scaling tests</li>
<li>Scaling test write-up</li>
</ul>
</section>
<section id="basics-of-parallelization" class="slide level2">
<h2>Basics of parallelization</h2>
<p>Serial v parallel: Which compute tasks are independent and would benefit from multiple processors?</p>
<p>Basic architecture (see also e.g., <a href="https://curc.readthedocs.io/en/latest/programming/parallel-programming-fundamentals.html">here</a>)</p>
<figure>
<img src="./parallel-arch.png" />
</figure>
</section>
<section id="section" class="slide level2">
<h2></h2>
<ul>
<li>A primer on the various jargon: node, core, processor, processes, tasks, etc.</li>
<li>See also <a href="https://smileipic.github.io/Smilei/parallelization.html">here</a></li>
</ul>
<figure>
<img src="NodesCoresThreads.png" />
</figure>
</section>
<section id="section-1" class="slide level2">
<h2></h2>
<p>Example: Stampede2 (see also <a href="https://docs.tacc.utexas.edu/hpc/stampede2/">documentation</a>)</p>
<ul>
<li>240 Intel &quot;Ice Lake&quot; (ICX) compute nodes, replacing 448 KNL compute nodes</li>
<li>Knights Landing (KNL) compute nodes each have 68 cores</li>
<li>Skylake (SKX) compute nodes each have 48 cores</li>
<li><a href="https://slurm.schedmd.com/quickstart.html">SLURM job scheduler</a></li>
<li>Note: Stampede2 will retire November 30, 2023</li>
</ul>
<figure>
<img src="knl-arch-stampede2.png" />
</figure>
</section>
<section id="section-2" class="slide level2">
<h2></h2>
<p>Example: Lonestar6 (see also <a href="https://docs.tacc.utexas.edu/hpc/lonestar6/">documentation</a>)</p>
<ul>
<li>560 AMD Milan compute nodes, each 128 cores on two sockets (64 cores/socket)</li>
<li>32 GPU nodes with NVIDIA A100</li>
</ul>
<figure>
<img src="lonestar6-arch.png" />
</figure>
</section>
<section id="software-modules-on-the-supercomputer" class="slide level2">
<h2>Software modules on the supercomputer</h2>
<ul>
<li><a href="https://docs.tacc.utexas.edu/software/quantumespresso/">QE at TACC</a></li>
</ul>
<figure>
<img src="QE-at-TACC.png" />
</figure>
<ul>
<li><a href="https://docs.tacc.utexas.edu/software/vasp/">VASP at TACC</a></li>
</ul>
<figure>
<img src="VASP-at-TACC.png" />
</figure>
</section>
<section id="jobs-on-the-supercomputer" class="slide level2">
<h2>Jobs on the supercomputer</h2>
<p>Example: Accessing TACC resources</p>
<ul>
<li>Will depend on which machine you are using.</li>
</ul>
<div class="sourceCode"><pre class="sourceCode bash"><code class="sourceCode bash"><span class="fu">ssh</span> -X <span class="op">&lt;</span>your-username<span class="op">&gt;</span>@stampede2.tacc.utexas.edu</code></pre></div>
<div class="sourceCode"><pre class="sourceCode bash"><code class="sourceCode bash"><span class="fu">ssh</span> -X <span class="op">&lt;</span>you-username<span class="op">&gt;</span>@ls6.tacc.utexas.edu</code></pre></div>
</section>
<section id="section-3" class="slide level2">
<h2></h2>
<div class="sourceCode"><pre class="sourceCode bash"><code class="sourceCode bash"><span class="ex">The</span> authenticity of host <span class="st">&#39;stampede2.tacc.utexas.edu (129.114.63.44)&#39;</span> can<span class="st">&#39;t be established.</span>
<span class="st">ECDSA key fingerprint is SHA256:SegC2YyyftiRpdwhXqNZE+15RyGeFSal4Vuz0HYJ5E8.</span>
<span class="st">Are you sure you want to continue connecting (yes/no/[fingerprint])? yes</span>
<span class="st">Warning: Permanently added &#39;</span>stampede2.tacc.utexas.edu,129.114.63.44<span class="st">&#39; (ECDSA) to the list of known hosts.</span>

<span class="st">To access the system:</span>

<span class="st">1) If not using ssh-keys, please enter your TACC password at the password prompt</span>
<span class="st">2) At the TACC Token prompt, enter your 6-digit code followed by &lt;return&gt;.</span>
<span class="st">Password: </span>
<span class="st">TACC Token Code:</span>
<span class="st">Last login: Thu Feb 14 11:37:13 2019 from 149.165.168.51</span>
<span class="st">------------------------------------------------------------------------------</span>
<span class="st">                   Welcome to the Stampede2 Supercomputer</span>
<span class="st">      Texas Advanced Computing Center, The University of Texas at Austin</span>
<span class="st">------------------------------------------------------------------------------</span>

<span class="st">              ** Unauthorized use/access is prohibited. **</span>

<span class="st">If you log on to this computer system, you acknowledge your awareness</span>
<span class="st">of and concurrence with the UT Austin Acceptable Use Policy. The</span>
<span class="st">University will prosecute violators to the full extent of the law.</span>

<span class="st">TACC Usage Policies:</span>
<span class="st">http://www.tacc.utexas.edu/user-services/usage-policies/</span>
<span class="st">______________________________________________________________________________</span>

<span class="st">Welcome to Stampede2, *please* read these important system notes:</span>

<span class="st">--&gt; Stampede2 user documentation is available at:</span>
<span class="st">       https://portal.tacc.utexas.edu/user-guides/stampede2</span>

<span class="st">2/24/2022:  OS was updated to latest CentOS 7.9 release along with Slurm 20,</span>
<span class="st">            OPA 10.11 and Lustre 2.12 updates.  Please submit a support ticket</span>
<span class="st">            if you encounter any issues after the update.</span>

<span class="st">            The icx-normal queue is now available for users, up to 40 nodes</span>
<span class="st">            with 80 cores per node for a single job.</span>
<span class="st">--------------------- Project balances for user wwwennie ----------------------</span>
<span class="st">| Name           Avail SUs     Expires |                                      |</span>
<span class="st">| TG-MAT220010        1600  2023-04-06 |                                      |</span>
<span class="st">------------------------ Disk quotas for user wwwennie ------------------------</span>
<span class="st">| Disk         Usage (GB)     Limit    %Used   File Usage       Limit   %Used |</span>
<span class="st">| /home1              2.4      10.0    24.34         3541      200000    1.77 |</span>
<span class="st">| /work2              0.0    1024.0     0.00            3     3000000    0.00 |</span>
<span class="st">| /scratch            0.0       0.0     0.00           66           0    0.00 |</span>
<span class="st">-------------------------------------------------------------------------------</span></code></pre></div>
</section>
<section id="jobs-on-the-supercomputer-1" class="slide level2">
<h2>Jobs on the supercomputer</h2>
<p>Example: <a href="https://docs.tacc.utexas.edu/software/quantumespresso/">Stampede2 at TACC</a></p>
<ul>
<li>see also <code>job.script</code></li>
</ul>
<div class="sourceCode"><pre class="sourceCode bash"><code class="sourceCode bash"><span class="co">#!/bin/bash </span>
<span class="co">#SBATCH -J qe                               # define the job name</span>
<span class="co">#SBATCH -o qe.%j.out                        # define stdout &amp; stderr output files </span>
<span class="co">#SBATCH -e qe.%j.err </span>
<span class="co">#SBATCH -N 4                                # request 4 nodes </span>
<span class="co">#SBATCH -n 256                              # 256 total tasks = 64 tasks/node</span>
<span class="co">#SBATCH -p normal                           # submit to &quot;normal&quot; queue </span>
<span class="co">#SBATCH -t 4:00:00                          # run for 4 hours max </span>
<span class="co">#SBATCH -A projectname</span>

<span class="ex">module</span> load qe/6.2.1                        # setup environment
<span class="ex">ibrun</span> pw.x -input qeinput <span class="op">&gt;</span> qe_test.out     # launch job</code></pre></div>
<div class="sourceCode"><pre class="sourceCode bash"><code class="sourceCode bash"><span class="ex">sbatch</span> job.script</code></pre></div>
</section>
<section id="parallelization-in-dft-codes" class="slide level2">
<h2>Parallelization in DFT codes</h2>
<ul>
<li>Every code has a scheme of parallelization- it is important to understand what these levels are and how well they perform</li>
<li>The goal is to load balance so that no cores are unnecessarily idle</li>
</ul>
</section>
<section id="parallelization-in-dft-codes-1" class="slide level2">
<h2>Parallelization in DFT codes</h2>
<p>Example: Quantum ESPRESSO</p>
<ul>
<li>Parallelize 8 images across 512 processors; for each image:</li>
<li>2 pools of over k-points with 256 processors;</li>
<li>4 task groups for the 3D FFT with 64 processors each</li>
<li>144 processes for diagonalization of subspace Hamiltonian<br />
</li>
<li>512 processors/image * 8 images = 4096 processors requested in total</li>
</ul>
<div class="sourceCode"><pre class="sourceCode bash"><code class="sourceCode bash"><span class="ex">mpirun</span> -np 4096 ./neb.x -ni 8 -nk 2 -nt 4 -nd 144 -i my.input</code></pre></div>
</section>
<section id="what-are-scaling-tests" class="slide level2">
<h2>What are scaling tests</h2>
<p>Goal: find most efficient parallelization scheme</p>
<p>How do you choose the parallelization parameters?</p>
<p>--&gt; Documentation/forums, tutorials and small test cases, and scaling tests!</p>
<p>Scaling tests: estimating resource usage; in this case, based on small tests of parallelization schemes</p>
<p>TACC measures compute sources in terms of <a href="https://www.tacc.utexas.edu/use-tacc/allocations/">Service Units (SUs)</a></p>
<p><code>SUs charged = (Number of nodes) x (job wall-clock time) x (multiplier, which depends on machine)</code></p>
</section>
<section id="why-we-do-scaling-tests" class="slide level2">
<h2>Why we do scaling tests</h2>
<ul>
<li><p>DFT codes scale with <span class="math inline">\(O(N^3)\)</span> where <span class="math inline">\(N\)</span> is some measure of system size (e.g., number of electrons, number of bands, number of k-points)</p></li>
<li><p>In general, not all parts of a DFT calculation scales linearly with processors (<a href="https://hpc-wiki.info/hpc/Scaling">general scaling in HPC</a>)</p></li>
<li><p>A simulation cell of about 100 atoms could take anywhere between a few hours to a few weeks (e.g., depending on functional)</p></li>
</ul>
</section>
<section id="why-we-do-scaling-tests-1" class="slide level2">
<h2>Why we do scaling tests</h2>
<ul>
<li><p>To find the most efficient and optimal running scheme!</p></li>
<li><p>Not enough parallelization: wait too long for calculation to finish</p></li>
<li><p>Improper parallelization: idle processors, waste of hours, waste of energy! (see also e.g., <a href="https://www.green-algorithms.org/GA4HPC/">Green Algorithms 4 HPC</a></p></li>
</ul>
</section>
<section id="how-do-we-do-scaling-tests" class="slide level2">
<h2>How do we do scaling tests</h2>
<ol type="1">
<li><p>set up a working calculation that is representative of the types of calculations you anticipate for the project within the next allocation period (usually 6 months to a year).</p></li>
<li><p>enumerate a few parallelization schemes and set up your calculations</p></li>
<li>run the minimum calculation needed</li>
</ol>
<ul>
<li>e.g., run a single or a few scf cycles, estimate number of scf cycles required to run ionic relaxations</li>
<li>e.g., run single q-point phonon calculation, extrapolate to get total estimated time to run full q-point mesh</li>
</ul>
<ol start="4" type="1">
<li>collect data on timings, plot the timings, and pick a scheme that is close to linear scaling but does not substantially waste resources</li>
</ol>
</section>
<section id="example-scaling-test" class="slide level2">
<h2>Example Scaling test</h2>
<ul>
<li>Comparing WO<span class="math inline">\(_3\)</span> and SrTiO<span class="math inline">\(_3\)</span> electron transport properties</li>
<li>do tests with and without spin-orbit coupling (SOC)</li>
<li>need to compute many phonons on a dense grid</li>
<li>find out time to compute single phonon; repeat for with and without SOC</li>
<li>Paper: <a href="https://dx.doi.org/10.1103/PhysRevB.101.045116">doi:10.1103/PhysRevB.101.045116</a></li>
</ul>
</section>
<section id="example-scaling-test-1" class="slide level2">
<h2>Example: Scaling test</h2>
<ul>
<li>A basic way to set up the calculations for running each parallelization scheme</li>
</ul>
<p>set_scaling.sh</p>
<div class="sourceCode"><pre class="sourceCode bash"><code class="sourceCode bash"><span class="co">#!/bin/bash</span>

<span class="va">proc=(</span>144 160 320<span class="va">)</span>  # <span class="ex">number</span> of requested processors
<span class="va">npool=(</span>2 4 6 8<span class="va">)</span>     # <span class="ex">using</span> k-point parallelization in QE

<span class="co"># loop over each possible combination of processors</span>
<span class="co"># and k-point parallelization</span>
<span class="co"># make a separate directory p&lt;processors&gt;n&lt;pools&gt;</span>
<span class="co"># generate correpsonding input files and job submission script</span>
<span class="kw">for</span> <span class="ex">i</span> in <span class="st">&quot;</span><span class="va">${proc[@]}</span><span class="st">&quot;</span><span class="kw">;</span> <span class="kw">do</span>
   <span class="kw">for</span> <span class="ex">j</span> in <span class="st">&quot;</span><span class="va">${npool[@]}</span><span class="st">&quot;</span><span class="kw">;</span> <span class="kw">do</span>
       <span class="fu">mkdir</span> p<span class="va">${i}</span>n<span class="va">${j}</span><span class="kw">;</span> <span class="bu">cd</span> p<span class="va">${i}</span>n<span class="va">${j}</span>
       <span class="co"># copy template input files</span>
       <span class="fu">cp</span> -r ../input.in ../*UPF ../phrun.sh .
       <span class="co"># substitute parallelization into run command </span>
       <span class="co"># of a template job submission script called phrun.sh</span>
       <span class="fu">sed</span> -i <span class="st">&quot;s/REPL1/</span><span class="va">${i}</span><span class="st">/g&quot;</span> phrun.sh<span class="kw">;</span> <span class="fu">sed</span> -i <span class="st">&quot;s/REPL2/</span><span class="va">${j}</span><span class="st">/g&quot;</span> phrun.sh
       <span class="bu">cd</span> ../
   <span class="kw">done</span>
<span class="kw">done</span></code></pre></div>
<div class="sourceCode"><pre class="sourceCode bash"><code class="sourceCode bash"><span class="ex">wwwennie@stampede2</span> scaling-test $ ls
<span class="ex">p144n2</span>  p144n4  p144n6  p144n8  p160n2  p160n4  p160n6  p160n8  p320n2  p320n4  p320n6  p320n8  set_scaling.sh</code></pre></div>
</section>
<section id="example-scaling-test-2" class="slide level2">
<h2>Example: Scaling test</h2>
<ul>
<li>Collect the total wall-clock time for a single phonon and plot</li>
<li>we plot scaling 1/time v number of processors</li>
<li>In ideal scaling, doubling the number of processors would halve the wall-clock time</li>
<li>In reality, at some point, running with more processors often leads to sub-linear scaling</li>
<li>One potential cause for sub-linear scaling is increased overhead of communication between interconnects, especially for large memory jobs</li>
<li>In this case, we chose to use the parallelization scheme with 28 processors. While the scaling is sub-linear, the added speed in calculations was deemed worthwhile given the dense q-point grids this calculation would be repeated over (i.e., some computational efficiency was traded for human-time speed).</li>
</ul>
<figure>
<img src="cubic-elph.png" />
</figure>
</section>
<section id="writing-up-scaling-tests-in-allocation-proposals" class="slide level2">
<h2>Writing up scaling tests in allocation proposals</h2>
<ul>
<li>Progress Report:</li>
<li>Preliminary results, papers published with allocation</li>
<li>Scientific Research Objective</li>
<li>What is the goal of the calculation?</li>
<li>Describe the project</li>
<li>Computational Methodology</li>
<li>What methods will you use?</li>
<li>Are the proposed methods appropriate and feasible?</li>
<li>How will HPC resources play a role in your calculations?</li>
<li>Applications/Codes: Will the software require any special setup?</li>
<li>Application efficiencies: Scaling tests</li>
<li>Computational Research Plan</li>
<li>What types of calculations are planned for the proposed allocation?</li>
<li>Justification of computer SUs and memory requirements</li>
</ul>
<p>Each HPC center has its own allocation cycle and policies (e.g., at <a href="https://allocations.access-ci.org/prepare-requests-overview">TACC</a>)</p>
</section>
<section id="writing-up-scaling-tests" class="slide level2">
<h2>Writing up scaling tests</h2>
<ul>
<li>aim to do tests on anticipated machine architecture</li>
<li>add some buffer to the estimated hours requested; there will be failed jobs, testing of parameters not known at time of scaling tests, etc.</li>
</ul>
<p><a href="https://utexas.box.com/s/v66fdky6slqvqc87i6mc7ydks43mp4yx">Group Box Example writeup</a>; must have UT EID</p>
</section>
    </div>
  </div>

  <script src="https://unpkg.com/reveal.js@3.9.2//lib/js/head.min.js"></script>
  <script src="https://unpkg.com/reveal.js@3.9.2//js/reveal.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({

        // Optional reveal.js plugins
        dependencies: [
          { src: 'https://unpkg.com/reveal.js@3.9.2//lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: 'https://unpkg.com/reveal.js@3.9.2//plugin/zoom-js/zoom.js', async: true },
              { src: 'https://unpkg.com/reveal.js@3.9.2//plugin/notes/notes.js', async: true }
        ]
      });
    </script>
    </body>
</html>
